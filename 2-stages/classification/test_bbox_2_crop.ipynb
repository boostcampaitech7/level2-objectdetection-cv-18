{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    PredictionString       image_id\n",
      "0  7 0.8395863175392151 215.65679931640625 50.585...  test/0000.jpg\n",
      "1  4 0.6536312103271484 344.48748779296875 249.65...  test/0001.jpg\n",
      "2  1 0.7134121656417847 424.9185485839844 269.755...  test/0002.jpg\n",
      "3  9 0.5041786432266235 139.1294708251953 264.084...  test/0003.jpg\n",
      "4  0 0.5496219396591187 426.969482421875 407.7453...  test/0004.jpg\n"
     ]
    }
   ],
   "source": [
    "origin_df = pd.read_csv('/home/hwang/leem/level2-objectdetection-cv-18/2-stages/level1-imageclassification-cv-18/post-processing/WBF_ATSS.csv')\n",
    "print(origin_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        image_id                                          bbox_info\n",
      "0  test/0000.jpg  [[7, 0.8395863175392151, 215.65679931640625, 5...\n",
      "1  test/0001.jpg  [[4, 0.6536312103271484, 344.48748779296875, 2...\n",
      "2  test/0002.jpg  [[1, 0.7134121656417847, 424.9185485839844, 26...\n",
      "3  test/0003.jpg  [[9, 0.5041786432266235, 139.1294708251953, 26...\n",
      "4  test/0004.jpg  [[0, 0.5496219396591187, 426.969482421875, 407...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# CSV 파일 로드\n",
    "df = pd.read_csv('/home/hwang/leem/level2-objectdetection-cv-18/2-stages/level1-imageclassification-cv-18/post-processing/WBF_ATSS.csv')\n",
    "\n",
    "# PredictionString을 6개 단위로 나누는 함수 정의\n",
    "def split_prediction_string(prediction_string):\n",
    "    # 공백으로 나누어 리스트로 변환\n",
    "    split_values = prediction_string.split()\n",
    "    \n",
    "    # 6개 단위로 묶어서 반환 (class_id, confidence_score, x1, y1, width, height)\n",
    "    return [split_values[i:i+6] for i in range(0, len(split_values), 6)]\n",
    "\n",
    "root_dir = \"/hdd1/lim_data/level2_dataset/test\"\n",
    "# 각 PredictionString에 대해 함수 적용하여 bbox_info 열 추가\n",
    "# df['image_id'] = df['image_id'].apply(lambda x: os.path.join(root_dir, x))\n",
    "df['bbox_info'] = df['PredictionString'].apply(split_prediction_string)\n",
    "\n",
    "# 결과 출력\n",
    "print(df[['image_id', 'bbox_info']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_new_id = {\n",
    "    'General trash': 0,\n",
    "    'Paper': 1,\n",
    "    'Paper pack': 2,\n",
    "    'Metal': 3,\n",
    "    'Glass': 4,\n",
    "    'Plastic': 5,\n",
    "    'Styrofoam': 6,\n",
    "    'Plastic bag': 7,\n",
    "    'Battery': 8,\n",
    "    'Clothing': 9\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/hdd1/lim_data/level2_dataset/crop_ATSS_test'\n",
    "os.makedirs(output_dir, exist_ok=True)  # 출력 디렉토리가 없으면 생성\n",
    "\n",
    "cropped_data = []\n",
    "def crop_and_save_images(image_id, bbox_info_list):\n",
    "    # 이미지 파일 경로 설정\n",
    "    image_path = os.path.join(\"/hdd1/lim_data/level2_dataset\", image_id)  # 'your_image_folder'에 실제 이미지가 위치\n",
    "    img = Image.open(image_path)  # 이미지 로드\n",
    "\n",
    "    # 각 bounding box에 대해 크롭 후 저장\n",
    "    for idx, bbox_info in enumerate(bbox_info_list):\n",
    "        class_id, confidence, x1, y1, width, height = map(float, bbox_info)\n",
    "        x1, y1, width, height = int(x1), int(y1), int(width), int(height)\n",
    "\n",
    "        # Bounding box로 이미지 크롭\n",
    "        cropped_img = img.crop((x1, y1, x1 + width, y1 + height))\n",
    "\n",
    "        # 크롭된 이미지 저장 (파일 이름에 image_id와 bbox 번호 포함)\n",
    "        cropped_img_name = f\"{os.path.splitext(os.path.basename(image_id))[0]}_crop_{idx}.jpg\"\n",
    "        cropped_img_path = os.path.join(output_dir, cropped_img_name)\n",
    "        cropped_img.save(cropped_img_path)\n",
    "\n",
    "        cropped_data.append({\n",
    "            'label': int(class_id),  # label에 class_id를 임시로 저장\n",
    "            'new_image_path': f\"crop_test/{cropped_img_name}\",\n",
    "            'label_id': int(class_id),\n",
    "            'cs' : confidence\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.apply(lambda row: crop_and_save_images(row['image_id'], row['bbox_info']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4871/4871 [26:51<00:00,  3.02it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    crop_and_save_images(row['image_id'], row['bbox_info'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame으로 변환 후, class_id를 label로 변환\n",
    "cropped_df = pd.DataFrame(cropped_data)\n",
    "\n",
    "# class_id를 label로 변환\n",
    "cropped_df['label'] = cropped_df['label'].replace({v: k for k, v in class_to_new_id.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         label             new_image_path  label_id        cs\n",
      "0  Plastic bag  crop_test/0000_crop_0.jpg         7  0.839586\n",
      "1  Plastic bag  crop_test/0000_crop_1.jpg         7  0.793338\n",
      "2        Paper  crop_test/0000_crop_2.jpg         1  0.773484\n",
      "3  Plastic bag  crop_test/0000_crop_3.jpg         7  0.771402\n",
      "4  Plastic bag  crop_test/0000_crop_4.jpg         7  0.762092\n"
     ]
    }
   ],
   "source": [
    "csv_name = \"/hdd1/lim_data/level2_dataset/csv/WBF_ATSS_test.csv\"\n",
    "cropped_df.to_csv(csv_name, index=False)\n",
    "print(cropped_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def save_bbox_info_to_csv(df, output_csv_path):\n",
    "    # CSV에 저장할 데이터를 담을 리스트\n",
    "    cropped_data = []\n",
    "    \n",
    "    # 각 row에 대해 bbox 정보를 저장\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        image_id = row['image_id']\n",
    "        bbox_info_list = row['bbox_info']\n",
    "\n",
    "        for idx, bbox_info in enumerate(bbox_info_list):\n",
    "            class_id, confidence, x1, y1, width, height = map(float, bbox_info)\n",
    "\n",
    "            cropped_data.append({\n",
    "                'image_id': os.path.basename(image_id),  # image_id의 base 경로만 저장\n",
    "                'idx' : idx,\n",
    "                'class_id': int(class_id),\n",
    "                'confidence': confidence,\n",
    "                'x1': x1,\n",
    "                'y1': y1,\n",
    "                'width': width,\n",
    "                'height': height\n",
    "            })\n",
    "\n",
    "    # DataFrame으로 변환 후 CSV로 저장\n",
    "    cropped_df = pd.DataFrame(cropped_data)\n",
    "    cropped_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "    return cropped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4871/4871 [00:00<00:00, 7035.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   image_id  idx  class_id  confidence          x1          y1       width  \\\n",
      "0  0000.jpg    0         7    0.839586  215.656799   50.585876  455.092651   \n",
      "1  0000.jpg    1         7    0.793338  448.197571  602.263062  647.875061   \n",
      "2  0000.jpg    2         1    0.773484  554.609985  104.805801  755.877747   \n",
      "3  0000.jpg    3         7    0.771402  604.898438  517.767456  957.673035   \n",
      "4  0000.jpg    4         7    0.762092  390.148102  191.706436  611.041016   \n",
      "\n",
      "        height  \n",
      "0   471.122375  \n",
      "1   875.052856  \n",
      "2   360.105255  \n",
      "3  1022.789124  \n",
      "4   546.604614  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "output_csv_path = \"/home/hwang/leem/level2-objectdetection-cv-18/2-stages/classification/WBF_ATSS_bbox_info.csv\"\n",
    "convert_df = save_bbox_info_to_csv(df, output_csv_path)\n",
    "print(convert_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_prediction_string_format(cropped_df, output_csv_path):\n",
    "    # 새로 변환할 데이터를 담을 리스트\n",
    "    prediction_data = []\n",
    "    \n",
    "    # image_id를 기준으로 그룹화하여 PredictionString을 생성\n",
    "    grouped = cropped_df.groupby('image_id')\n",
    "    \n",
    "    for image_id, group in grouped:\n",
    "        # 각 이미지에 대한 bbox 정보들을 PredictionString으로 결합\n",
    "        prediction_string = \" \".join(\n",
    "            group.apply(lambda row: f\"{int(row['class_id'])} {row['confidence']} {int(row['x1'])} {int(row['y1'])} {int(row['width'])} {int(row['height'])}\", axis=1)\n",
    "        )\n",
    "        \n",
    "        prediction_data.append({\n",
    "            'PredictionString': prediction_string,\n",
    "            'image_id': image_id\n",
    "            \n",
    "        })\n",
    "    \n",
    "    # DataFrame으로 변환 후 CSV로 저장\n",
    "    prediction_df = pd.DataFrame(prediction_data)\n",
    "    prediction_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "    return prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    PredictionString  image_id\n",
      "0  7 0.8395863175392151 215 50 455 471 7 0.793337...  0000.jpg\n",
      "1  4 0.6536312103271484 344 249 753 695 5 0.56312...  0001.jpg\n",
      "2  1 0.7134121656417847 424 269 641 325 0 0.66376...  0002.jpg\n",
      "3  9 0.5041786432266235 139 264 893 820 9 0.25488...  0003.jpg\n",
      "4  0 0.5496219396591187 426 407 656 574 1 0.35824...  0004.jpg\n"
     ]
    }
   ],
   "source": [
    "# 앞서 저장한 bbox 정보 CSV 파일을 불러오거나 만든 DataFrame을 사용\n",
    "cropped_df = pd.read_csv(\"/home/hwang/leem/level2-objectdetection-cv-18/2-stages/classification/WBF_ATSS_bbox_info.csv\")\n",
    "output_csv_path = \"/home/hwang/leem/level2-objectdetection-cv-18/2-stages/classification/converted_WBF_ATSS.csv\"\n",
    "\n",
    "# 분리된 데이터를 다시 PredictionString 형식으로 변환\n",
    "prediction_df = convert_to_prediction_string_format(cropped_df, output_csv_path)\n",
    "\n",
    "# 변환된 데이터 확인\n",
    "print(prediction_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 두 CSV 파일의 컬럼 이름과 순서가 동일합니다.\n",
      "✅ 두 CSV 파일의 크기(행과 열 개수)가 동일합니다. (크기: (4871, 2))\n",
      "✅ 첫 번째 열의 문자열 형식(공백 개수)이 두 파일에서 동일합니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compare_csv_format(file1_path, file2_path):\n",
    "    # 두 CSV 파일 로드\n",
    "    df1 = pd.read_csv(file1_path)\n",
    "    df2 = pd.read_csv(file2_path)\n",
    "    \n",
    "    # 1. 컬럼 이름과 순서 비교\n",
    "    columns_file1 = df1.columns.tolist()\n",
    "    columns_file2 = df2.columns.tolist()\n",
    "    \n",
    "    if columns_file1 != columns_file2:\n",
    "        print(\"❌ 두 CSV 파일의 컬럼 이름과 순서가 다릅니다.\")\n",
    "        print(f\"파일 1의 컬럼: {columns_file1}\")\n",
    "        print(f\"파일 2의 컬럼: {columns_file2}\")\n",
    "        return\n",
    "    else:\n",
    "        print(\"✅ 두 CSV 파일의 컬럼 이름과 순서가 동일합니다.\")\n",
    "    \n",
    "    # 2. 행(row)과 열(column) 개수 비교\n",
    "    if df1.shape != df2.shape:\n",
    "        print(f\"❌ 두 CSV 파일의 크기가 다릅니다. (파일 1: {df1.shape}, 파일 2: {df2.shape})\")\n",
    "        return\n",
    "    else:\n",
    "        print(f\"✅ 두 CSV 파일의 크기(행과 열 개수)가 동일합니다. (크기: {df1.shape})\")\n",
    "    \n",
    "    # 3. 첫 번째 열의 문자열 형식 비교 (공백 개수 확인)\n",
    "    first_column_file1 = df1.iloc[:, 0].astype(str)\n",
    "    first_column_file2 = df2.iloc[:, 0].astype(str)\n",
    "    \n",
    "    mismatch_count = 0\n",
    "    \n",
    "    for i in range(len(first_column_file1)):\n",
    "        space_count_file1 = first_column_file1[i].count(' ')\n",
    "        space_count_file2 = first_column_file2[i].count(' ')\n",
    "        \n",
    "        if space_count_file1 != space_count_file2:\n",
    "            print(f\"❌ 파일 1과 파일 2의 첫 번째 열에서 공백 개수가 다릅니다. (행 {i + 1})\")\n",
    "            print(f\"파일 1: '{first_column_file1[i]}' (공백 {space_count_file1}개)\")\n",
    "            print(f\"파일 2: '{first_column_file2[i]}' (공백 {space_count_file2}개)\")\n",
    "            mismatch_count += 1\n",
    "            \n",
    "    if mismatch_count == 0:\n",
    "        print(\"✅ 첫 번째 열의 문자열 형식(공백 개수)이 두 파일에서 동일합니다.\")\n",
    "    else:\n",
    "        print(f\"총 {mismatch_count}개의 행에서 공백 개수가 다릅니다.\")\n",
    "    \n",
    "# 사용 예시\n",
    "file1_path = \"/home/hwang/leem/level2-objectdetection-cv-18/2-stages/classification/converted_WBF_ATSS.csv\"\n",
    "file2_path = \"/home/hwang/leem/level2-objectdetection-cv-18/2-stages/level1-imageclassification-cv-18/post-processing/WBF_ATSS.csv\"\n",
    "compare_csv_format(file1_path, file2_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eva",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
